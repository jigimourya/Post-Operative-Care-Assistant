{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **1. 🩺 Introduction**\n",
        "\n",
        "The Post-Operative Care Assistant is an AI-powered healthcare application designed to assist in monitoring and evaluating patients’ post-surgery recovery. By analyzing patient data and generating automated evaluation summaries, the system helps medical professionals make informed decisions and provide timely care."
      ],
      "metadata": {
        "id": "KCJf3SltxVeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Import Dependencies**"
      ],
      "metadata": {
        "id": "Ysna7V86xFvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core Python libraries\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import hashlib\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# Data and computation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# AI/ML\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "\n",
        "# Vector storage\n",
        "import faiss\n",
        "\n",
        "# PDF processing\n",
        "import PyPDF2\n",
        "\n",
        "# Interface\n",
        "import gradio as gr\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "0yy0pJ2XhjmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Load and Prepare Dataset**\n",
        "Read or load medical/EHR text data, preprocess it, and chunk for embeddings."
      ],
      "metadata": {
        "id": "3EElX4foxAX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilities\n",
        "\n",
        "def sha1_str(s: str) -> str:\n",
        "    return hashlib.sha1(s.encode('utf-8')).hexdigest()\n",
        "\n",
        "def simple_clean(text: str) -> str:\n",
        "    # Normalize newlines and spaces\n",
        "    t = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
        "    # Remove lines with only numbers (like page numbers)\n",
        "    t = '\\n'.join([line for line in t.split('\\n') if not re.fullmatch(r'\\s*\\d+\\s*', line)])\n",
        "    # Collapse multiple newlines\n",
        "    t = re.sub(r'\\n{3,}', '\\n\\n', t)\n",
        "    # Collapse multiple spaces\n",
        "    t = re.sub(r'[ \\t]{2,}', ' ', t)\n",
        "    return t.strip()\n",
        "\n",
        "def chunk_text_by_words(text: str, chunk_words: int = 300, overlap: int = 50):\n",
        "    words = text.split()\n",
        "    if len(words) <= chunk_words:\n",
        "        return [' '.join(words)]\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    step = chunk_words - overlap\n",
        "    while i < len(words):\n",
        "        chunk = words[i:i+chunk_words]\n",
        "        chunks.append(' '.join(chunk))\n",
        "        i += step\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "0gtaIvpnhprH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ingest Documents\n",
        "\n",
        "def load_pdf_file(path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(path, \"rb\") as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            for page in reader.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Failed to read PDF {path}: {e}\")\n",
        "    return text\n",
        "\n",
        "def ingest_documents_from_dir(input_dir):\n",
        "    SUPPORTED_EXT = {\".txt\", \".md\", \".csv\", \".pdf\"}\n",
        "    docs = []\n",
        "    for p in sorted(Path(input_dir).rglob(\"*\")):\n",
        "        if p.is_file() and p.suffix.lower() in SUPPORTED_EXT:\n",
        "            try:\n",
        "                if p.suffix.lower() == \".csv\":\n",
        "                    df = pd.read_csv(p)\n",
        "                    text_col = next((c for c in ['text','content','body','summary'] if c in df.columns), None)\n",
        "                    if text_col is None:\n",
        "                        continue\n",
        "                    for idx, row in df.iterrows():\n",
        "                        txt = str(row[text_col])\n",
        "                        docs.append({\n",
        "                            'doc_id': sha1_str(str(p) + str(idx)),\n",
        "                            'title': row.get('title', f\"{p.name}-{idx}\"),\n",
        "                            'source': str(p),\n",
        "                            'date': row.get('date', ''),\n",
        "                            'text': txt\n",
        "                        })\n",
        "                elif p.suffix.lower() == \".pdf\":\n",
        "                    txt = load_pdf_file(p)\n",
        "                    docs.append({\n",
        "                        'doc_id': sha1_str(str(p)),\n",
        "                        'title': p.stem,\n",
        "                        'source': str(p),\n",
        "                        'date': '',\n",
        "                        'text': txt\n",
        "                    })\n",
        "                else:\n",
        "                    txt = load_txt_file(p)\n",
        "                    docs.append({\n",
        "                        'doc_id': sha1_str(str(p)),\n",
        "                        'title': p.stem,\n",
        "                        'source': str(p),\n",
        "                        'date': '',\n",
        "                        'text': txt\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to read {p}: {e}\")\n",
        "    return docs"
      ],
      "metadata": {
        "id": "7-RUUYZghqPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess & Chunk\n",
        "\n",
        "def preprocess_and_chunk(docs, chunk_words=300, overlap=50):\n",
        "    chunks = []\n",
        "    for d in docs:\n",
        "        cleaned = simple_clean(d.get('text',''))\n",
        "        text_chunks = chunk_text_by_words(cleaned, chunk_words, overlap)\n",
        "        for i, c in enumerate(text_chunks):\n",
        "            chunk_id = sha1_str(d['doc_id'] + str(i))\n",
        "            chunks.append({\n",
        "                'doc_id': d['doc_id'],\n",
        "                'chunk_id': chunk_id,\n",
        "                'title': d.get('title',''),\n",
        "                'source': d.get('source',''),\n",
        "                'date': d.get('date',''),\n",
        "                'chunk_text': c,\n",
        "                'chunk_word_count': len(c.split()),\n",
        "                'created_at': datetime.utcnow().isoformat()+'Z'\n",
        "            })\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "glRtFdoNhsB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Embedding and FAISS Indexing**\n",
        "Convert text chunks to embeddings, store in FAISS, and save the index."
      ],
      "metadata": {
        "id": "eRAjlyydyBYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings\n",
        "\n",
        "class EmbeddingEngine:\n",
        "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "    def embed(self, texts):\n",
        "        return self.model.encode(texts, show_progress_bar=True, convert_to_numpy=True).astype('float32')"
      ],
      "metadata": {
        "id": "WlLwotlQhtW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build FAISS Index\n",
        "\n",
        "def build_faiss_index(embeddings):\n",
        "    if not HAS_FAISS:\n",
        "        print(\"FAISS not installed. Skipping index creation.\")\n",
        "        return None\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "    return index"
      ],
      "metadata": {
        "id": "98BQGACMhu1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Phase 1\n",
        "\n",
        "def run_phase1_colab(input_dir, chunk_words=300, overlap=50):\n",
        "    print(\"[INFO] Ingesting documents...\")\n",
        "    docs = ingest_documents_from_dir(input_dir)\n",
        "    print(f\"[INFO] Found {len(docs)} documents.\")\n",
        "\n",
        "    print(\"[INFO] Preprocessing and chunking...\")\n",
        "    chunks = preprocess_and_chunk(docs, chunk_words, overlap)\n",
        "    print(f\"[INFO] Created {len(chunks)} chunks.\")\n",
        "\n",
        "    print(\"[INFO] Computing embeddings...\")\n",
        "    texts = [c['chunk_text'] for c in chunks]\n",
        "    emb_engine = EmbeddingEngine()\n",
        "    embeddings = emb_engine.embed(texts)\n",
        "    print(f\"[INFO] Embeddings shape: {embeddings.shape}\")\n",
        "\n",
        "    print(\"[INFO] Building FAISS index...\")\n",
        "    index = build_faiss_index(embeddings)\n",
        "    if index:\n",
        "        print(f\"[INFO] FAISS index built with {index.ntotal} vectors.\")\n",
        "\n",
        "    # Save outputs to Google Colab local runtime (optional)\n",
        "    os.makedirs('processed_data', exist_ok=True)\n",
        "    pd.DataFrame(chunks).to_csv('processed_data/chunks.csv', index=False)\n",
        "    np.save('processed_data/embeddings.npy', embeddings)\n",
        "    with open('processed_data/metadata.json','w') as f:\n",
        "        json.dump(chunks, f, indent=2)\n",
        "    print(\"[INFO] Phase 1 complete. Outputs saved in processed_data/ folder.\")"
      ],
      "metadata": {
        "id": "NaA_TmMMhzUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_phase1_colab('data_docs')"
      ],
      "metadata": {
        "id": "MQmkPOr9h0ya",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Retrieval-Augmented Generation (RAG) Pipeline**\n",
        "Retrieve top-k context chunks and generate responses with LLM."
      ],
      "metadata": {
        "id": "7dLCOPQWye-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stored embeddings + metadata\n",
        "print(\"[INFO] Loading stored embeddings and metadata...\")\n",
        "\n",
        "chunks_df = pd.read_csv('processed_data/chunks.csv')\n",
        "embeddings = np.load('processed_data/embeddings.npy')\n",
        "\n",
        "with open('processed_data/metadata.json', 'r') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "print(f\"[INFO] Loaded {len(chunks_df)} chunks with embeddings of shape {embeddings.shape}\")"
      ],
      "metadata": {
        "id": "9sEY4gyWyZVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load embedding model\n",
        "\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "print(f\"[INFO] Loading embedding model: {EMBEDDING_MODEL}\")\n",
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL)"
      ],
      "metadata": {
        "id": "AhmXb3G57yUG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model.to(device)"
      ],
      "metadata": {
        "id": "djsvVskIC-WZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build FAISS index\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(embeddings)\n",
        "print(f\"[INFO] FAISS index built with {index.ntotal} vectors of dimension {dim}\")"
      ],
      "metadata": {
        "id": "n_Fk626B8HCi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define retrieval function\n",
        "def retrieve_top_docs(query, top_k=3, metadata_filter=None):\n",
        "    \"\"\"\n",
        "    Retrieves top_k relevant chunks for a user query.\n",
        "    Optionally applies metadata filtering.\n",
        "    \"\"\"\n",
        "    # Embed the user query\n",
        "    query_emb = embedding_model.encode([query])\n",
        "\n",
        "    # Perform similarity search\n",
        "    D, I = index.search(query_emb, top_k)\n",
        "\n",
        "    results = []\n",
        "    for idx, score in zip(I[0], D[0]):\n",
        "        if idx >= len(chunks_df):\n",
        "            continue\n",
        "        chunk_info = chunks_df.iloc[idx].to_dict()\n",
        "\n",
        "        if metadata_filter:\n",
        "            valid = all(chunk_info.get(k) == v for k, v in metadata_filter.items())\n",
        "            if not valid:\n",
        "                continue\n",
        "\n",
        "        results.append({\n",
        "            \"content\": chunk_info[\"chunk_text\"],\n",
        "            \"title\": chunk_info.get(\"title\", \"\"),\n",
        "            \"source\": chunk_info.get(\"source\", \"\"),\n",
        "            \"similarity_score\": float(score)\n",
        "        })\n",
        "    return results"
      ],
      "metadata": {
        "id": "cg4191en8L1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Retrieval Pipeline\n",
        "\n",
        "sample_query = \"How should I clean my surgical incision after knee surgery?\"\n",
        "results = retrieve_top_docs(sample_query, top_k=3)\n",
        "\n",
        "print(\"\\n[INFO] 🔍 Retrieval Results:\\n\")\n",
        "for i, res in enumerate(results, start=1):\n",
        "    print(f\"Result {i}:\")\n",
        "    print(f\"Title: {res['title']}\")\n",
        "    print(f\"Source: {res['source']}\")\n",
        "    print(f\"Similarity Score: {res['similarity_score']:.4f}\")\n",
        "    print(f\"Chunk:\\n{res['content'][:400]}...\\n\")"
      ],
      "metadata": {
        "id": "cZo4XmQR8OuI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "# 📘Context Augmentation & LLM Generation\n",
        "# ================================================"
      ],
      "metadata": {
        "id": "WihXPyzF9Cdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your open-source LLM\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "print(\"[INFO] 🔄 Loading LLM model and tokenizer...\")\n",
        "llm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(\"[INFO] ✅ Model Loaded Successfully!\")\n"
      ],
      "metadata": {
        "id": "2Zh-oizTD9LT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=llm_model,\n",
        "    tokenizer=llm_tokenizer,\n",
        "    max_new_tokens=300,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "Qgp9ycpKG5d3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧩 Prompt Template\n",
        "\n",
        "def build_prompt(user_query, retrieved_docs, ehr_info=None):\n",
        "    context_text = \"\\n\\n\".join(\n",
        "        [f\"Source: {doc['source']}\\nContent: {doc['content']}\" for doc in retrieved_docs]\n",
        "    )\n",
        "\n",
        "    ehr_context = \"\"\n",
        "    if ehr_info:\n",
        "        ehr_context = (\n",
        "            f\"Surgery Type: {ehr_info.get('surgery_type', 'N/A')}\\n\"\n",
        "            f\"Allergies: {ehr_info.get('allergies', 'N/A')}\\n\"\n",
        "            f\"Recovery Timeline: {ehr_info.get('recovery_timeline', 'N/A')}\\n\"\n",
        "        )\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a post-operative care assistant.\n",
        "Use the context to provide an accurate and safe medical response.\n",
        "\n",
        "Patient Information:\n",
        "{ehr_context}\n",
        "\n",
        "Patient Query: {user_query}\n",
        "\n",
        "Context Information:\n",
        "{context_text}\n",
        "\n",
        "Provide a concise, safe, and medically appropriate response:\n",
        "\"\"\"\n",
        "    return prompt.strip()"
      ],
      "metadata": {
        "id": "hsaoqNk7AWST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧠 Response Generation\n",
        "\n",
        "def generate_response(prompt, max_new_tokens=300, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Generate only the new model response (not the full prompt).\n",
        "    \"\"\"\n",
        "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\").to(llm_model.device)\n",
        "    output_tokens = llm_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=llm_tokenizer.eos_token_id\n",
        "    )\n",
        "    full_text = llm_tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "    response_only = full_text[len(prompt):].strip()\n",
        "\n",
        "    return response_only"
      ],
      "metadata": {
        "id": "GsUINwM2JAGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧩 Connect Retrieval + Generation\n",
        "\n",
        "def answer_patient_query(user_query, ehr_info=None, top_k=3):\n",
        "    \"\"\"\n",
        "    Retrieves docs + patient info, builds prompt, and returns clean model output.\n",
        "    \"\"\"\n",
        "    retrieved_docs = retrieve_top_docs(user_query, top_k=top_k)\n",
        "    full_prompt = build_prompt(user_query, retrieved_docs, ehr_info)\n",
        "\n",
        "    # Generate concise answer\n",
        "    response_text = generate_response(full_prompt)\n",
        "    return response_text"
      ],
      "metadata": {
        "id": "556lRryxJAGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Example test\n",
        "# -------------------------------\n",
        "ehr_example = {\n",
        "    \"surgery_type\": \"Total Knee Replacement\",\n",
        "    \"allergies\": \"Penicillin\",\n",
        "    \"recovery_timeline\": \"2 weeks post-op\"\n",
        "}\n",
        "\n",
        "user_query = \"How should I clean my surgical incision after knee surgery?\"\n",
        "response = answer_patient_query(user_query, ehr_info=ehr_example, top_k=3)\n",
        "\n",
        "print(\"\\n[INFO] 🩺 Final AI Response:\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "iHNJFaM7JAGH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Response Evaluation and Safety Filtering**\n",
        "Categorize responses and flag unsafe/incomplete ones."
      ],
      "metadata": {
        "id": "qd3lwSY8zxZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔹 Output Delivery & Source Linking\n",
        "\n",
        "def display_patient_response(final_text):\n",
        "    \"\"\"\n",
        "    Pretty print markdown formatted output.\n",
        "    \"\"\"\n",
        "    display(Markdown(final_text))\n",
        "\n",
        "# Example visualization\n",
        "display_patient_response(response)\n",
        "\n",
        "# Categorization logic\n",
        "def categorize_response(response_text):\n",
        "    if \"🚨\" in response_text:\n",
        "        return \"🚨 ER Visit\"\n",
        "    elif \"⚠️\" in response_text:\n",
        "        return \"⚠️ Provider Connect\"\n",
        "    else:\n",
        "        return \"✅ AI Recommendation\"\n",
        "\n",
        "category = categorize_response(response)\n",
        "print(f\"\\n[INFO] Recommendation Category: {category}\")\n"
      ],
      "metadata": {
        "id": "lQaX-wPs_nFt",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📊 Response Evaluation & Logging\n",
        "\n",
        "def evaluate_response(user_query, response, retrieved_docs):\n",
        "    \"\"\"\n",
        "    Performs a simple heuristic evaluation of the model output\n",
        "    for post-operative medical guidance.\n",
        "    Returns a dictionary with quality scores and flags.\n",
        "    \"\"\"\n",
        "\n",
        "    eval_result = {\n",
        "        \"query\": user_query,\n",
        "        \"response_length\": len(response.split()),\n",
        "        \"has_warning_signs\": any(keyword in response.lower() for keyword in [\"infection\", \"bleeding\", \"emergency\", \"immediately\"]),\n",
        "        \"contains_context_terms\": any(doc[\"source\"].split('/')[-1].replace('.pdf', '').lower() in response.lower() for doc in retrieved_docs),\n",
        "        \"is_concise\": len(response.split()) < 200,\n",
        "        \"is_safe_tone\": not any(term in response.lower() for term in [\"ignore\", \"disregard\", \"self-medicate\", \"skip doctor\"]),\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Heuristic summary\n",
        "    eval_result[\"summary\"] = \"✅ Safe & relevant\" if eval_result[\"is_safe_tone\"] and eval_result[\"is_concise\"] else \"⚠️ Needs review\"\n",
        "\n",
        "    return eval_result\n",
        "\n",
        "\n",
        "def log_interaction(user_query, ehr_info, response, eval_result, save_path=\"interaction_logs.json\"):\n",
        "    \"\"\"\n",
        "    Logs query, patient info, response, and evaluation results to a JSON file.\n",
        "    \"\"\"\n",
        "\n",
        "    log_entry = {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"query\": user_query,\n",
        "        \"ehr_info\": ehr_info,\n",
        "        \"response\": response,\n",
        "        \"evaluation\": eval_result\n",
        "    }\n",
        "\n",
        "    # If log file exists, append new entry; otherwise create new\n",
        "    if os.path.exists(save_path):\n",
        "        with open(save_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "    else:\n",
        "        data = []\n",
        "\n",
        "    data.append(log_entry)\n",
        "\n",
        "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "    print(f\"[LOGGED] Interaction saved to {save_path}\")"
      ],
      "metadata": {
        "id": "YHpYGLo1IoVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "#  Example Test\n",
        "# -------------------------------\n",
        "user_query = \"How should I clean my surgical incision after knee surgery?\"\n",
        "\n",
        "response = answer_patient_query(user_query, ehr_info=ehr_example, top_k=3)\n",
        "\n",
        "# Evaluate response quality\n",
        "eval_result = evaluate_response(user_query, response, retrieve_top_docs(user_query))\n",
        "\n",
        "# Log everything\n",
        "log_interaction(user_query, ehr_example, response, eval_result)\n",
        "\n",
        "print(\"\\n[INFO] ✅ Evaluation Summary:\")\n",
        "print(json.dumps(eval_result, indent=4))"
      ],
      "metadata": {
        "id": "eJiuRwvSKnY4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🛡️ Automated Safety Guardrail\n",
        "\n",
        "def apply_safety_guardrail(response):\n",
        "    \"\"\"\n",
        "    Enhanced contextual guardrail that checks for unsafe or misleading advice.\n",
        "    Allows safe educational phrasing (e.g., 'do not skip antibiotics').\n",
        "    \"\"\"\n",
        "    text = response.lower()\n",
        "\n",
        "    # Absolute unsafe triggers (always blocked)\n",
        "    unsafe_terms = [\n",
        "        \"self-medicate\", \"double dose\", \"stop taking without\",\n",
        "        \"ignore your doctor\", \"use unprescribed\", \"apply heat directly\"\n",
        "    ]\n",
        "\n",
        "    # Contextually risky terms (flag only if not negated)\n",
        "    caution_terms = [\"skip\", \"avoid\", \"delay\"]\n",
        "\n",
        "    # Detect strict unsafe content\n",
        "    flagged_terms = [term for term in unsafe_terms if term in text]\n",
        "\n",
        "    # Context-aware detection for 'skip'\n",
        "    for term in caution_terms:\n",
        "        # Example: \"skip antibiotics\" (unsafe), but not \"do not skip antibiotics\"\n",
        "        unsafe_pattern = rf\"\\b((?!do not|don’t|never).)\\b{term}\\b.*(medicine|dose|antibiotic|exercise|therapy)\"\n",
        "        if re.search(unsafe_pattern, text):\n",
        "            flagged_terms.append(term)\n",
        "\n",
        "    if flagged_terms:\n",
        "        print(f\"⚠️ [GUARDRAIL] Potentially unsafe or misleading phrasing detected: {flagged_terms}\")\n",
        "        safe_response = (\n",
        "            \"⚠️ This response may include ambiguous phrasing. \"\n",
        "            \"Please follow only your doctor's instructions or seek clarification from your healthcare provider.\"\n",
        "        )\n",
        "        return safe_response, True\n",
        "    else:\n",
        "        return response, False\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Example Integration\n",
        "# -------------------------------\n",
        "user_query = \"Can I skip my antibiotics if I feel better after surgery?\"\n",
        "ehr_info = {\n",
        "    \"surgery_type\": \"Appendectomy\",\n",
        "    \"allergies\": \"None\",\n",
        "    \"recovery_timeline\": \"3 days post-op\"\n",
        "}\n",
        "\n",
        "# Generate model answer\n",
        "model_response = answer_patient_query(user_query, ehr_info=ehr_info)\n",
        "\n",
        "# Apply safety guardrail\n",
        "safe_response, flagged = apply_safety_guardrail(model_response)\n",
        "\n",
        "# Evaluate and log\n",
        "eval_result = evaluate_response(user_query, safe_response, retrieve_top_docs(user_query))\n",
        "log_interaction(user_query, ehr_info, safe_response, eval_result)\n",
        "\n",
        "print(\"\\n[INFO] 🩺 Final Safe Response:\")\n",
        "print(safe_response)\n",
        "print(f\"\\n[FLAGGED]: {flagged}\")\n"
      ],
      "metadata": {
        "id": "MkFvb_vGL4XR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Gradio Interface**\n",
        "Display clean UI for user inputs and AI-generated outputs."
      ],
      "metadata": {
        "id": "aPlCn5qKjOPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gradio_postop_assistant(user_query, surgery_type, allergies, recovery_timeline, top_k=3):\n",
        "    \"\"\"\n",
        "    Main function to connect user input to retrieval + LLM + safety guardrail\n",
        "    \"\"\"\n",
        "    ehr_info = {\n",
        "        \"surgery_type\": surgery_type,\n",
        "        \"allergies\": allergies,\n",
        "        \"recovery_timeline\": recovery_timeline\n",
        "    }\n",
        "\n",
        "    # 1️⃣ Generate model response\n",
        "    model_response = answer_patient_query(user_query, ehr_info=ehr_info, top_k=top_k)\n",
        "\n",
        "    # 2️⃣ Apply safety guardrail\n",
        "    safe_response, flagged = apply_safety_guardrail(model_response)\n",
        "\n",
        "    # 3️⃣ Evaluate response\n",
        "    eval_result = evaluate_response(user_query, safe_response, retrieve_top_docs(user_query, top_k=top_k))\n",
        "\n",
        "    # 4️⃣ Log interaction\n",
        "    log_interaction(user_query, ehr_info, safe_response, eval_result)\n",
        "\n",
        "    # 5️⃣ Categorize\n",
        "    category = categorize_response(safe_response)\n",
        "\n",
        "    # 6️⃣ Prepare output\n",
        "    output_text = f\"**Response Category:** {category}\\n\\n**Response:**\\n{safe_response}\"\n",
        "\n",
        "    return output_text, flagged, json.dumps(eval_result, indent=2)\n",
        "\n",
        "\n",
        "# Gradio UI Components\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 🩺 Post-Operative Care Assistant\")\n",
        "    gr.Markdown(\n",
        "        \"Enter your patient query and relevant surgical/EHR information. \"\n",
        "        \"The assistant will retrieve context, generate a safe response, \"\n",
        "        \"and categorize it.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            user_query = gr.Textbox(label=\"Patient Query\", placeholder=\"Describe your question...\")\n",
        "            surgery_type = gr.Textbox(label=\"Surgery Type\", placeholder=\"e.g., Total Knee Replacement\")\n",
        "            allergies = gr.Textbox(label=\"Allergies\", placeholder=\"e.g., Penicillin\")\n",
        "            recovery_timeline = gr.Textbox(label=\"Recovery Timeline\", placeholder=\"e.g., 2 weeks post-op\")\n",
        "            top_k = gr.Slider(1, 10, value=3, step=1, label=\"Number of Context Chunks to Retrieve\")\n",
        "            submit_btn = gr.Button(\"Get Advice\")\n",
        "\n",
        "        with gr.Column():\n",
        "            response_output = gr.Markdown(label=\"Assistant Response\")\n",
        "            flagged_output = gr.Textbox(label=\"⚠️ Flagged?\", interactive=False)\n",
        "            eval_output = gr.Textbox(label=\"Evaluation Details\", lines=10, max_lines=20, interactive=False, placeholder=\"Evaluation summary will appear here...\")\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=gradio_postop_assistant,\n",
        "        inputs=[user_query, surgery_type, allergies, recovery_timeline, top_k],\n",
        "        outputs=[response_output, flagged_output, eval_output]\n",
        "    )\n",
        "\n",
        "## 8. Run App\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "WSFvcWAuXT7S",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}